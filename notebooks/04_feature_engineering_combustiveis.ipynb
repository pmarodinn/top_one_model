{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5cab9a1b",
   "metadata": {},
   "source": [
    "# Feature Engineering - Dados de Combustíveis por Região\n",
    "\n",
    "**Projeto:** Top One Model - Sistema de Modelagem de Risco de Crédito\n",
    "\n",
    "**Objetivo:**\n",
    "- Processar e organizar os dados brutos de combustíveis da ANP\n",
    "- Criar estrutura de dados padronizada e consistente\n",
    "- Separar informações por tempo, região, cidade e valores\n",
    "- Preparar dados para implementação futura do modelo de risco\n",
    "\n",
    "**Dados de Entrada:**\n",
    "```\n",
    "data/external_data/macro_specified_data/fuel_prices/raw_data/\n",
    "├── resumo/     # Dados consolidados por região\n",
    "└── revendas/   # Dados detalhados por posto\n",
    "```\n",
    "\n",
    "**Dados de Saída:**\n",
    "```\n",
    "data/external_data/macro_specified_data/fuel_prices/processed_data/\n",
    "├── resumo_consolidado.parquet\n",
    "├── revendas_consolidado.parquet\n",
    "└── indices_regionais.parquet\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cfe0299",
   "metadata": {},
   "source": [
    "## 1. Configuração Inicial e Importações"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6ae80cf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔧 FEATURE ENGINEERING - DADOS DE COMBUSTÍVEIS\n",
      "============================================================\n",
      "📅 Data de execução: 04/08/2025 18:05:58\n",
      "🎯 Objetivo: Processar e organizar dados para o modelo\n",
      "✅ Bibliotecas importadas com sucesso!\n"
     ]
    }
   ],
   "source": [
    "# Importações essenciais\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timedelta\n",
    "import re\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Para processamento paralelo e otimização\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import glob\n",
    "\n",
    "print(\"🔧 FEATURE ENGINEERING - DADOS DE COMBUSTÍVEIS\")\n",
    "print(\"=\" * 60)\n",
    "print(\"📅 Data de execução:\", datetime.now().strftime('%d/%m/%Y %H:%M:%S'))\n",
    "print(\"🎯 Objetivo: Processar e organizar dados para o modelo\")\n",
    "print(\"✅ Bibliotecas importadas com sucesso!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaa1dec4",
   "metadata": {},
   "source": [
    "## 2. Configuração de Caminhos e Estrutura"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5b35eb2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📁 ESTRUTURA DE DADOS CONFIGURADA:\n",
      "   📂 Raw Data Resumo: /home/usuario/Documentos/top_one_model_01/data/external_data/macro_specified_data/fuel_prices/raw_data/resumo\n",
      "   📂 Raw Data Revendas: /home/usuario/Documentos/top_one_model_01/data/external_data/macro_specified_data/fuel_prices/raw_data/revendas\n",
      "   📂 Processed Data: /home/usuario/Documentos/top_one_model_01/data/external_data/macro_specified_data/fuel_prices/processed_data\n",
      "\n",
      "📊 DADOS DISPONÍVEIS:\n",
      "   • Resumos: 140 arquivos\n",
      "   • Revendas: 139 arquivos\n",
      "✅ Estrutura verificada com sucesso!\n"
     ]
    }
   ],
   "source": [
    "# Configuração das pastas\n",
    "project_root = Path('/home/usuario/Documentos/top_one_model_01')\n",
    "fuel_prices_root = project_root / 'data' / 'external_data' / 'macro_specified_data' / 'fuel_prices'\n",
    "\n",
    "# Pastas de dados\n",
    "raw_data_path = fuel_prices_root / 'raw_data'\n",
    "processed_data_path = fuel_prices_root / 'processed_data'\n",
    "\n",
    "# Subpastas de raw data\n",
    "pasta_resumo = raw_data_path / 'resumo'\n",
    "pasta_revendas = raw_data_path / 'revendas'\n",
    "\n",
    "# Criar pasta de dados processados se não existir\n",
    "processed_data_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"📁 ESTRUTURA DE DADOS CONFIGURADA:\")\n",
    "print(f\"   📂 Raw Data Resumo: {pasta_resumo}\")\n",
    "print(f\"   📂 Raw Data Revendas: {pasta_revendas}\")\n",
    "print(f\"   📂 Processed Data: {processed_data_path}\")\n",
    "\n",
    "# Verificar dados disponíveis\n",
    "arquivos_resumo = list(pasta_resumo.glob('*.xlsx'))\n",
    "arquivos_revendas = list(pasta_revendas.glob('*.xlsx'))\n",
    "\n",
    "print(f\"\\n📊 DADOS DISPONÍVEIS:\")\n",
    "print(f\"   • Resumos: {len(arquivos_resumo)} arquivos\")\n",
    "print(f\"   • Revendas: {len(arquivos_revendas)} arquivos\")\n",
    "print(\"✅ Estrutura verificada com sucesso!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47ff56ee",
   "metadata": {},
   "source": [
    "## 3. Análise Inicial da Estrutura dos Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f9b972b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 ANÁLISE ESTRUTURAL DOS DADOS\n",
      "==================================================\n",
      "\n",
      "📋 ANÁLISE DOS RESUMOS (primeiros 3 arquivos):\n",
      "\n",
      "📄 1. resumo_2022-08-21_2022-08-27.xlsx\n",
      "   ❌ Erro: ExternalReference.__init__() missing 1 required positional argument: 'id'\n",
      "\n",
      "📄 2. resumo_2022-08-28_2022-09-03.xlsx\n",
      "   ❌ Erro: ExternalReference.__init__() missing 1 required positional argument: 'id'\n",
      "\n",
      "📄 3. resumo_2022-09-04_2022-09-10.xlsx\n",
      "   📊 Shape: 181 linhas x 12 colunas\n",
      "   🗺️ Colunas geográficas: 0 encontradas\n",
      "   💰 Colunas de preço: 0 encontradas\n",
      "   ⛽ Colunas de combustível: 0 encontradas\n",
      "\n",
      "🏪 ANÁLISE DAS REVENDAS (primeiros 3 arquivos):\n",
      "\n",
      "📄 1. revendas_2022-09-04_2022-09-10.xlsx\n",
      "   ❌ Erro: ExternalReference.__init__() missing 1 required positional argument: 'id'\n",
      "\n",
      "📄 2. revendas_2022-09-11_2022-09-17.xlsx\n",
      "   ❌ Erro: ExternalReference.__init__() missing 1 required positional argument: 'id'\n",
      "\n",
      "📄 3. revendas_2022-09-25_2022-10-01.xlsx\n",
      "   ❌ Erro: ExternalReference.__init__() missing 1 required positional argument: 'id'\n",
      "\n",
      "✅ ANÁLISE ESTRUTURAL CONCLUÍDA!\n"
     ]
    }
   ],
   "source": [
    "def analisar_estrutura_arquivo(arquivo_path):\n",
    "    \"\"\"\n",
    "    Analisa a estrutura de um arquivo Excel de combustíveis\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Ler arquivo\n",
    "        df = pd.read_excel(arquivo_path, engine='openpyxl')\n",
    "        \n",
    "        # Extrair informações básicas\n",
    "        info = {\n",
    "            'arquivo': arquivo_path.name,\n",
    "            'shape': df.shape,\n",
    "            'colunas': list(df.columns),\n",
    "            'colunas_count': len(df.columns),\n",
    "            'tipos_dados': df.dtypes.to_dict(),\n",
    "            'primeiras_linhas': df.head(3).to_dict('records'),\n",
    "            'valores_nulos': df.isnull().sum().to_dict(),\n",
    "            'erro': None\n",
    "        }\n",
    "        \n",
    "        # Identificar colunas importantes\n",
    "        colunas_geo = [col for col in df.columns if any(termo in str(col).lower() for termo in ['estado', 'uf', 'município', 'regiao', 'cidade'])]\n",
    "        colunas_preco = [col for col in df.columns if any(termo in str(col).lower() for termo in ['preço', 'preco', 'valor', 'media'])]\n",
    "        colunas_combustivel = [col for col in df.columns if any(termo in str(col).lower() for termo in ['gasolina', 'etanol', 'diesel', 'gnv'])]\n",
    "        \n",
    "        info['colunas_geograficas'] = colunas_geo\n",
    "        info['colunas_preco'] = colunas_preco\n",
    "        info['colunas_combustivel'] = colunas_combustivel\n",
    "        \n",
    "        return info\n",
    "        \n",
    "    except Exception as e:\n",
    "        return {\n",
    "            'arquivo': arquivo_path.name,\n",
    "            'erro': str(e)\n",
    "        }\n",
    "\n",
    "# Analisar alguns arquivos de cada tipo\n",
    "print(\"🔍 ANÁLISE ESTRUTURAL DOS DADOS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "print(\"\\n📋 ANÁLISE DOS RESUMOS (primeiros 3 arquivos):\")\n",
    "for i, arquivo in enumerate(sorted(arquivos_resumo)[:3]):\n",
    "    info = analisar_estrutura_arquivo(arquivo)\n",
    "    print(f\"\\n📄 {i+1}. {info['arquivo']}\")\n",
    "    \n",
    "    if info.get('erro'):\n",
    "        print(f\"   ❌ Erro: {info['erro']}\")\n",
    "    else:\n",
    "        print(f\"   📊 Shape: {info['shape'][0]:,} linhas x {info['shape'][1]} colunas\")\n",
    "        print(f\"   🗺️ Colunas geográficas: {len(info['colunas_geograficas'])} encontradas\")\n",
    "        print(f\"   💰 Colunas de preço: {len(info['colunas_preco'])} encontradas\")\n",
    "        print(f\"   ⛽ Colunas de combustível: {len(info['colunas_combustivel'])} encontradas\")\n",
    "\n",
    "print(\"\\n🏪 ANÁLISE DAS REVENDAS (primeiros 3 arquivos):\")\n",
    "for i, arquivo in enumerate(sorted(arquivos_revendas)[:3]):\n",
    "    info = analisar_estrutura_arquivo(arquivo)\n",
    "    print(f\"\\n📄 {i+1}. {info['arquivo']}\")\n",
    "    \n",
    "    if info.get('erro'):\n",
    "        print(f\"   ❌ Erro: {info['erro']}\")\n",
    "    else:\n",
    "        print(f\"   📊 Shape: {info['shape'][0]:,} linhas x {info['shape'][1]} colunas\")\n",
    "        print(f\"   🗺️ Colunas geográficas: {len(info['colunas_geograficas'])} encontradas\")\n",
    "        print(f\"   💰 Colunas de preço: {len(info['colunas_preco'])} encontradas\")\n",
    "        print(f\"   ⛽ Colunas de combustível: {len(info['colunas_combustivel'])} encontradas\")\n",
    "\n",
    "print(\"\\n✅ ANÁLISE ESTRUTURAL CONCLUÍDA!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c9368ef",
   "metadata": {},
   "source": [
    "## 4. Funções de Limpeza e Padronização"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "47ef0d94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Funções de limpeza e padronização criadas!\n"
     ]
    }
   ],
   "source": [
    "def extrair_periodo_do_nome(nome_arquivo):\n",
    "    \"\"\"\n",
    "    Extrai o período (data inicial e final) do nome do arquivo\n",
    "    \"\"\"\n",
    "    # Padrão: tipo_YYYY-MM-DD_YYYY-MM-DD.xlsx\n",
    "    match = re.search(r'(\\d{4}-\\d{2}-\\d{2})_(\\d{4}-\\d{2}-\\d{2})', nome_arquivo)\n",
    "    \n",
    "    if match:\n",
    "        data_inicio = pd.to_datetime(match.group(1))\n",
    "        data_fim = pd.to_datetime(match.group(2))\n",
    "        \n",
    "        return {\n",
    "            'data_inicio': data_inicio,\n",
    "            'data_fim': data_fim,\n",
    "            'ano': data_inicio.year,\n",
    "            'mes': data_inicio.month,\n",
    "            'semana_ano': data_inicio.isocalendar()[1],\n",
    "            'periodo_string': f\"{data_inicio.strftime('%Y-%m-%d')} a {data_fim.strftime('%Y-%m-%d')}\"\n",
    "        }\n",
    "    \n",
    "    return None\n",
    "\n",
    "def limpar_nomes_colunas(df):\n",
    "    \"\"\"\n",
    "    Limpa e padroniza nomes das colunas\n",
    "    \"\"\"\n",
    "    # Remover espaços extras e caracteres especiais\n",
    "    colunas_limpas = []\n",
    "    \n",
    "    for col in df.columns:\n",
    "        # Converter para string e limpar\n",
    "        col_limpa = str(col).strip()\n",
    "        \n",
    "        # Remover quebras de linha e espaços múltiplos\n",
    "        col_limpa = re.sub(r'\\s+', ' ', col_limpa)\n",
    "        \n",
    "        # Remover caracteres especiais no início/fim\n",
    "        col_limpa = re.sub(r'^[^a-zA-Z0-9]+|[^a-zA-Z0-9]+$', '', col_limpa)\n",
    "        \n",
    "        colunas_limpas.append(col_limpa)\n",
    "    \n",
    "    df.columns = colunas_limpas\n",
    "    return df\n",
    "\n",
    "def identificar_tipo_combustivel(nome_coluna):\n",
    "    \"\"\"\n",
    "    Identifica o tipo de combustível baseado no nome da coluna\n",
    "    \"\"\"\n",
    "    nome_lower = str(nome_coluna).lower()\n",
    "    \n",
    "    if 'gasolina' in nome_lower or 'gasolin' in nome_lower:\n",
    "        return 'gasolina'\n",
    "    elif 'etanol' in nome_lower or 'alcool' in nome_lower or 'álcool' in nome_lower:\n",
    "        return 'etanol'\n",
    "    elif 'diesel' in nome_lower:\n",
    "        return 'diesel'\n",
    "    elif 'gnv' in nome_lower or 'gás natural' in nome_lower:\n",
    "        return 'gnv'\n",
    "    else:\n",
    "        return 'outro'\n",
    "\n",
    "def limpar_valores_numericos(series):\n",
    "    \"\"\"\n",
    "    Limpa e converte valores numéricos (preços)\n",
    "    \"\"\"\n",
    "    # Converter para string primeiro\n",
    "    series_str = series.astype(str)\n",
    "    \n",
    "    # Remover símbolos de moeda e espaços\n",
    "    series_clean = series_str.str.replace(r'[R$\\s]', '', regex=True)\n",
    "    \n",
    "    # Trocar vírgula por ponto para decimal\n",
    "    series_clean = series_clean.str.replace(',', '.')\n",
    "    \n",
    "    # Converter para numérico\n",
    "    return pd.to_numeric(series_clean, errors='coerce')\n",
    "\n",
    "print(\"✅ Funções de limpeza e padronização criadas!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "635a2b5f",
   "metadata": {},
   "source": [
    "## 5. Processamento dos Dados de Resumo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a5951c22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 PROCESSANDO TODOS OS DADOS DE RESUMO (2022-2025)\n",
      "============================================================\n",
      "💡 Processando todos os arquivos disponíveis desde a primeira coleta...\n",
      "📋 Total de arquivos para processar: 140\n",
      "📅 Período estimado: resumo_2022-08-21_2022-08-27.xlsx até resumo_2025-07-27_2025-08-02.xlsx\n",
      "📄 [1/140] Processando: resumo_2022-08-21_2022-08-27.xlsx\n",
      "   ❌ Erro ao processar resumo_2022-08-21_2022-08-27.xlsx: ExternalReference.__init__() missing 1 required positional argument: 'id'\n",
      "   ❌ Falha no processamento\n",
      "📄 [2/140] Processando: resumo_2022-08-28_2022-09-03.xlsx\n",
      "   ❌ Erro ao processar resumo_2022-08-28_2022-09-03.xlsx: ExternalReference.__init__() missing 1 required positional argument: 'id'\n",
      "   ❌ Falha no processamento\n",
      "📄 [3/140] Processando: resumo_2022-09-04_2022-09-10.xlsx\n",
      "   ✅ Sucesso: 172 registros\n",
      "📄 [4/140] Processando: resumo_2022-09-11_2022-09-17.xlsx\n",
      "   ✅ Sucesso: 147 registros\n",
      "📄 [5/140] Processando: resumo_2022-09-18_2022-09-24.xlsx\n",
      "   ✅ Sucesso: 13 registros\n",
      "📄 [6/140] Processando: resumo_2022-09-25_2022-10-01.xlsx\n",
      "   ✅ Sucesso: 174 registros\n",
      "📄 [7/140] Processando: resumo_2022-10-30_2022-11-05.xlsx\n",
      "   ✅ Sucesso: 161 registros\n",
      "📄 [8/140] Processando: resumo_2022-11-06_2022-11-12.xlsx\n",
      "   ✅ Sucesso: 174 registros\n",
      "📄 [9/140] Processando: resumo_2022-11-20_2022-11-26.xlsx\n",
      "   ✅ Sucesso: 172 registros\n",
      "📄 [10/140] Processando: resumo_2022-12-04_2022-12-10.xlsx\n",
      "   ❌ Erro ao processar resumo_2022-12-04_2022-12-10.xlsx: ExternalReference.__init__() missing 1 required positional argument: 'id'\n",
      "   ❌ Falha no processamento\n",
      "📄 [11/140] Processando: resumo_2022-12-11_2022-12-17.xlsx\n",
      "   ❌ Erro ao processar resumo_2022-12-11_2022-12-17.xlsx: ExternalReference.__init__() missing 1 required positional argument: 'id'\n",
      "   ❌ Falha no processamento\n",
      "📄 [12/140] Processando: resumo_2022-12-18_2022-12-24.xlsx\n",
      "   ❌ Erro ao processar resumo_2022-12-18_2022-12-24.xlsx: ExternalReference.__init__() missing 1 required positional argument: 'id'\n",
      "   ❌ Falha no processamento\n",
      "📄 [13/140] Processando: resumo_2023-01-01_2023-01-07.xlsx\n",
      "   ✅ Sucesso: 162 registros\n",
      "📄 [14/140] Processando: resumo_2023-01-08_2023-01-14.xlsx\n",
      "   ✅ Sucesso: 172 registros\n",
      "📄 [15/140] Processando: resumo_2023-01-15_2023-01-21.xlsx\n",
      "   ✅ Sucesso: 175 registros\n",
      "📄 [16/140] Processando: resumo_2023-01-22_2023-01-28.xlsx\n",
      "   ✅ Sucesso: 175 registros\n",
      "📄 [17/140] Processando: resumo_2023-01-29_2023-02-04.xlsx\n",
      "   ✅ Sucesso: 168 registros\n",
      "📄 [18/140] Processando: resumo_2023-02-05_2023-02-11.xlsx\n",
      "   ✅ Sucesso: 170 registros\n",
      "📄 [19/140] Processando: resumo_2023-02-12_2023-02-18.xlsx\n",
      "   ✅ Sucesso: 173 registros\n",
      "📄 [20/140] Processando: resumo_2023-02-19_2023-02-25.xlsx\n",
      "   ✅ Sucesso: 167 registros\n",
      "\n",
      "   📊 Progresso: 20/140 (75.0% sucesso até agora)\n",
      "   💾 Registros acumulados: 2,375\n",
      "📄 [21/140] Processando: resumo_2023-02-26_2023-03-04.xlsx\n",
      "   ❌ Erro ao processar resumo_2023-02-26_2023-03-04.xlsx: ExternalReference.__init__() missing 1 required positional argument: 'id'\n",
      "   ❌ Falha no processamento\n",
      "📄 [22/140] Processando: resumo_2023-03-05_2023-03-11.xlsx\n",
      "   ❌ Erro ao processar resumo_2023-03-05_2023-03-11.xlsx: ExternalReference.__init__() missing 1 required positional argument: 'id'\n",
      "   ❌ Falha no processamento\n",
      "📄 [23/140] Processando: resumo_2023-03-12_2023-03-18.xlsx\n",
      "   ❌ Erro ao processar resumo_2023-03-12_2023-03-18.xlsx: ExternalReference.__init__() missing 1 required positional argument: 'id'\n",
      "   ❌ Falha no processamento\n",
      "📄 [24/140] Processando: resumo_2023-03-19_2023-03-25.xlsx\n",
      "   ❌ Erro ao processar resumo_2023-03-19_2023-03-25.xlsx: ExternalReference.__init__() missing 1 required positional argument: 'id'\n",
      "   ❌ Falha no processamento\n",
      "📄 [25/140] Processando: resumo_2023-03-26_2023-04-01.xlsx\n",
      "   ❌ Erro ao processar resumo_2023-03-26_2023-04-01.xlsx: ExternalReference.__init__() missing 1 required positional argument: 'id'\n",
      "   ❌ Falha no processamento\n",
      "📄 [26/140] Processando: resumo_2023-04-09_2023-04-15.xlsx\n",
      "   ❌ Erro ao processar resumo_2023-04-09_2023-04-15.xlsx: ExternalReference.__init__() missing 1 required positional argument: 'id'\n",
      "   ❌ Falha no processamento\n",
      "📄 [27/140] Processando: resumo_2023-05-07_2023-05-13.xlsx\n",
      "   ✅ Sucesso: 174 registros\n",
      "📄 [28/140] Processando: resumo_2023-05-14_2023-05-20.xlsx\n",
      "   ✅ Sucesso: 172 registros\n",
      "📄 [29/140] Processando: resumo_2023-05-21_2023-05-27.xlsx\n",
      "   ✅ Sucesso: 171 registros\n",
      "📄 [30/140] Processando: resumo_2023-05-28_2023-06-03.xlsx\n",
      "   ✅ Sucesso: 172 registros\n",
      "📄 [31/140] Processando: resumo_2023-06-04_2023-06-10.xlsx\n",
      "   ✅ Sucesso: 175 registros\n",
      "📄 [32/140] Processando: resumo_2023-06-11_2023-06-17.xlsx\n",
      "   ✅ Sucesso: 172 registros\n",
      "📄 [33/140] Processando: resumo_2023-06-18_2023-06-24.xlsx\n",
      "   ✅ Sucesso: 173 registros\n",
      "📄 [34/140] Processando: resumo_2023-06-25_2023-07-01.xlsx\n",
      "   ✅ Sucesso: 174 registros\n",
      "📄 [35/140] Processando: resumo_2023-07-02_2023-07-08.xlsx\n",
      "   ✅ Sucesso: 172 registros\n",
      "📄 [36/140] Processando: resumo_2023-07-09_2023-07-15.xlsx\n",
      "   ✅ Sucesso: 171 registros\n",
      "📄 [37/140] Processando: resumo_2023-07-16_2023-07-22.xlsx\n",
      "   ✅ Sucesso: 171 registros\n",
      "📄 [38/140] Processando: resumo_2023-07-23_2023-07-29.xlsx\n",
      "   ✅ Sucesso: 171 registros\n",
      "📄 [39/140] Processando: resumo_2023-07-30_2023-08-05.xlsx\n",
      "   ✅ Sucesso: 171 registros\n",
      "📄 [40/140] Processando: resumo_2023-08-06_2023-08-12.xlsx\n",
      "   ✅ Sucesso: 169 registros\n",
      "\n",
      "   📊 Progresso: 40/140 (72.5% sucesso até agora)\n",
      "   💾 Registros acumulados: 4,783\n",
      "📄 [41/140] Processando: resumo_2023-08-13_2023-08-19.xlsx\n",
      "   ✅ Sucesso: 175 registros\n",
      "📄 [42/140] Processando: resumo_2023-08-20_2023-08-26.xlsx\n",
      "   ✅ Sucesso: 170 registros\n",
      "📄 [43/140] Processando: resumo_2023-08-27_2023-09-02.xlsx\n",
      "   ✅ Sucesso: 172 registros\n",
      "📄 [44/140] Processando: resumo_2023-09-03_2023-09-09.xlsx\n",
      "   ✅ Sucesso: 171 registros\n",
      "📄 [45/140] Processando: resumo_2023-09-10_2023-09-16.xlsx\n",
      "   ✅ Sucesso: 167 registros\n",
      "📄 [46/140] Processando: resumo_2023-09-17_2023-09-23.xlsx\n",
      "   ✅ Sucesso: 166 registros\n",
      "📄 [47/140] Processando: resumo_2023-09-24_2023-09-30.xlsx\n",
      "   ✅ Sucesso: 171 registros\n",
      "📄 [48/140] Processando: resumo_2023-10-01_2023-10-07.xlsx\n",
      "   ✅ Sucesso: 172 registros\n",
      "📄 [49/140] Processando: resumo_2023-10-08_2023-10-14.xlsx\n",
      "   ✅ Sucesso: 170 registros\n",
      "📄 [50/140] Processando: resumo_2023-10-15_2023-10-21.xlsx\n",
      "   ✅ Sucesso: 163 registros\n",
      "📄 [51/140] Processando: resumo_2023-10-22_2023-10-28.xlsx\n",
      "   ✅ Sucesso: 172 registros\n",
      "📄 [52/140] Processando: resumo_2023-10-29_2023-11-04.xlsx\n",
      "   ✅ Sucesso: 174 registros\n",
      "📄 [53/140] Processando: resumo_2023-11-05_2023-11-11.xlsx\n",
      "   ✅ Sucesso: 170 registros\n",
      "📄 [54/140] Processando: resumo_2023-11-12_2023-11-18.xlsx\n",
      "   ✅ Sucesso: 169 registros\n",
      "📄 [55/140] Processando: resumo_2023-11-19_2023-11-25.xlsx\n",
      "   ✅ Sucesso: 167 registros\n",
      "📄 [56/140] Processando: resumo_2023-11-26_2023-12-02.xlsx\n",
      "   ✅ Sucesso: 165 registros\n",
      "📄 [57/140] Processando: resumo_2023-12-03_2023-12-09.xlsx\n",
      "   ✅ Sucesso: 172 registros\n",
      "📄 [58/140] Processando: resumo_2023-12-10_2023-12-16.xlsx\n",
      "   ✅ Sucesso: 172 registros\n",
      "📄 [59/140] Processando: resumo_2023-12-17_2023-12-23.xlsx\n",
      "   ✅ Sucesso: 170 registros\n",
      "📄 [60/140] Processando: resumo_2023-12-24_2023-12-30.xlsx\n",
      "   ✅ Sucesso: 170 registros\n",
      "\n",
      "   📊 Progresso: 60/140 (81.7% sucesso até agora)\n",
      "   💾 Registros acumulados: 8,181\n",
      "📄 [61/140] Processando: resumo_2024-01-07_2024-01-13.xlsx\n",
      "   ✅ Sucesso: 171 registros\n",
      "📄 [62/140] Processando: resumo_2024-01-14_2024-01-20.xlsx\n",
      "   ✅ Sucesso: 170 registros\n",
      "📄 [63/140] Processando: resumo_2024-01-21_2024-01-27.xlsx\n",
      "   ✅ Sucesso: 165 registros\n",
      "📄 [64/140] Processando: resumo_2024-01-28_2024-02-03.xlsx\n",
      "   ✅ Sucesso: 172 registros\n",
      "📄 [65/140] Processando: resumo_2024-02-04_2024-02-10.xlsx\n",
      "   ✅ Sucesso: 165 registros\n",
      "📄 [66/140] Processando: resumo_2024-02-11_2024-02-17.xlsx\n",
      "   ✅ Sucesso: 168 registros\n",
      "📄 [67/140] Processando: resumo_2024-02-18_2024-02-24.xlsx\n",
      "   ✅ Sucesso: 167 registros\n",
      "📄 [68/140] Processando: resumo_2024-02-25_2024-03-02.xlsx\n",
      "   ✅ Sucesso: 171 registros\n",
      "📄 [69/140] Processando: resumo_2024-03-03_2024-03-09.xlsx\n",
      "   ✅ Sucesso: 167 registros\n",
      "📄 [70/140] Processando: resumo_2024-03-10_2024-03-16.xlsx\n",
      "   ✅ Sucesso: 170 registros\n",
      "📄 [71/140] Processando: resumo_2024-03-17_2024-03-23.xlsx\n",
      "   ✅ Sucesso: 170 registros\n",
      "📄 [72/140] Processando: resumo_2024-03-24_2024-03-30.xlsx\n",
      "   ✅ Sucesso: 168 registros\n",
      "📄 [73/140] Processando: resumo_2024-03-31_2024-04-06.xlsx\n",
      "   ✅ Sucesso: 165 registros\n",
      "📄 [74/140] Processando: resumo_2024-04-07_2024-04-13.xlsx\n",
      "   ✅ Sucesso: 172 registros\n",
      "📄 [75/140] Processando: resumo_2024-04-21_2024-04-27.xlsx\n",
      "   ✅ Sucesso: 168 registros\n",
      "📄 [76/140] Processando: resumo_2024-04-28_2024-05-04.xlsx\n",
      "   ✅ Sucesso: 167 registros\n",
      "📄 [77/140] Processando: resumo_2024-05-05_2024-05-11.xlsx\n",
      "   ✅ Sucesso: 164 registros\n",
      "📄 [78/140] Processando: resumo_2024-05-12_2024-05-18.xlsx\n",
      "   ✅ Sucesso: 163 registros\n",
      "📄 [79/140] Processando: resumo_2024-05-19_2024-05-25.xlsx\n",
      "   ✅ Sucesso: 165 registros\n",
      "📄 [80/140] Processando: resumo_2024-05-26_2024-06-01.xlsx\n",
      "   ✅ Sucesso: 163 registros\n",
      "\n",
      "   📊 Progresso: 80/140 (86.2% sucesso até agora)\n",
      "   💾 Registros acumulados: 11,532\n",
      "📄 [81/140] Processando: resumo_2024-06-02_2024-06-08.xlsx\n",
      "   ✅ Sucesso: 169 registros\n",
      "📄 [82/140] Processando: resumo_2024-06-09_2024-06-15.xlsx\n",
      "   ✅ Sucesso: 173 registros\n",
      "📄 [83/140] Processando: resumo_2024-06-16_2024-06-22.xlsx\n",
      "   ✅ Sucesso: 163 registros\n",
      "📄 [84/140] Processando: resumo_2024-06-23_2024-06-29.xlsx\n",
      "   ✅ Sucesso: 166 registros\n",
      "📄 [85/140] Processando: resumo_2024-06-30_2024-07-06.xlsx\n",
      "   ✅ Sucesso: 165 registros\n",
      "📄 [86/140] Processando: resumo_2024-07-07_2024-07-13.xlsx\n",
      "   ✅ Sucesso: 165 registros\n",
      "📄 [87/140] Processando: resumo_2024-07-14_2024-07-20.xlsx\n",
      "   ✅ Sucesso: 166 registros\n",
      "📄 [88/140] Processando: resumo_2024-07-21_2024-07-27.xlsx\n",
      "   ✅ Sucesso: 172 registros\n",
      "📄 [89/140] Processando: resumo_2024-07-28_2024-08-03.xlsx\n",
      "   ✅ Sucesso: 160 registros\n",
      "📄 [90/140] Processando: resumo_2024-08-04_2024-08-10.xlsx\n",
      "   ✅ Sucesso: 172 registros\n",
      "📄 [91/140] Processando: resumo_2024-08-11_2024-08-17.xlsx\n",
      "   ✅ Sucesso: 166 registros\n",
      "📄 [92/140] Processando: resumo_2024-08-18_2024-08-24.xlsx\n",
      "   ✅ Sucesso: 171 registros\n",
      "📄 [93/140] Processando: resumo_2024-08-25_2024-08-31.xlsx\n",
      "   ✅ Sucesso: 164 registros\n",
      "📄 [94/140] Processando: resumo_2024-09-01_2024-09-07.xlsx\n",
      "   ✅ Sucesso: 172 registros\n",
      "📄 [95/140] Processando: resumo_2024-09-08_2024-09-14.xlsx\n",
      "   ✅ Sucesso: 172 registros\n",
      "📄 [96/140] Processando: resumo_2024-09-15_2024-09-21.xlsx\n",
      "   ✅ Sucesso: 171 registros\n",
      "📄 [97/140] Processando: resumo_2024-09-22_2024-09-28.xlsx\n",
      "   ✅ Sucesso: 170 registros\n",
      "📄 [98/140] Processando: resumo_2024-09-29_2024-10-05.xlsx\n",
      "   ✅ Sucesso: 170 registros\n",
      "📄 [99/140] Processando: resumo_2024-10-06_2024-10-12.xlsx\n",
      "   ✅ Sucesso: 169 registros\n",
      "📄 [100/140] Processando: resumo_2024-10-13_2024-10-19.xlsx\n",
      "   ✅ Sucesso: 170 registros\n",
      "\n",
      "   📊 Progresso: 100/140 (89.0% sucesso até agora)\n",
      "   💾 Registros acumulados: 14,898\n",
      "📄 [101/140] Processando: resumo_2024-10-20_2024-10-26.xlsx\n",
      "   ✅ Sucesso: 171 registros\n",
      "📄 [102/140] Processando: resumo_2024-10-27_2024-11-02.xlsx\n",
      "   ✅ Sucesso: 172 registros\n",
      "📄 [103/140] Processando: resumo_2024-11-03_2024-11-09.xlsx\n",
      "   ✅ Sucesso: 173 registros\n",
      "📄 [104/140] Processando: resumo_2024-11-10_2024-11-16.xlsx\n",
      "   ✅ Sucesso: 170 registros\n",
      "📄 [105/140] Processando: resumo_2024-11-17_2024-11-23.xlsx\n",
      "   ✅ Sucesso: 170 registros\n",
      "📄 [106/140] Processando: resumo_2024-11-24_2024-11-30.xlsx\n",
      "   ✅ Sucesso: 171 registros\n",
      "📄 [107/140] Processando: resumo_2024-12-01_2024-12-07.xlsx\n",
      "   ✅ Sucesso: 172 registros\n",
      "📄 [108/140] Processando: resumo_2024-12-08_2024-12-14.xlsx\n",
      "   ✅ Sucesso: 171 registros\n",
      "📄 [109/140] Processando: resumo_2024-12-15_2024-12-21.xlsx\n",
      "   ✅ Sucesso: 165 registros\n",
      "📄 [110/140] Processando: resumo_2024-12-29_2025-01-04.xlsx\n",
      "   ✅ Sucesso: 162 registros\n",
      "📄 [111/140] Processando: resumo_2025-01-05_2025-01-11.xlsx\n",
      "   ✅ Sucesso: 165 registros\n",
      "📄 [112/140] Processando: resumo_2025-01-12_2025-01-18.xlsx\n",
      "   ✅ Sucesso: 171 registros\n",
      "📄 [113/140] Processando: resumo_2025-01-19_2025-01-25.xlsx\n",
      "   ✅ Sucesso: 172 registros\n",
      "📄 [114/140] Processando: resumo_2025-01-26_2025-02-01.xlsx\n",
      "   ✅ Sucesso: 166 registros\n",
      "📄 [115/140] Processando: resumo_2025-02-02_2025-02-08.xlsx\n",
      "   ✅ Sucesso: 170 registros\n",
      "📄 [116/140] Processando: resumo_2025-02-09_2025-02-15.xlsx\n",
      "   ✅ Sucesso: 172 registros\n",
      "📄 [117/140] Processando: resumo_2025-02-16_2025-02-22.xlsx\n",
      "   ✅ Sucesso: 170 registros\n",
      "📄 [118/140] Processando: resumo_2025-02-23_2025-03-01.xlsx\n",
      "   ✅ Sucesso: 166 registros\n",
      "📄 [119/140] Processando: resumo_2025-03-02_2025-03-08.xlsx\n",
      "   ✅ Sucesso: 169 registros\n",
      "📄 [120/140] Processando: resumo_2025-03-09_2025-03-15.xlsx\n",
      "   ✅ Sucesso: 172 registros\n",
      "\n",
      "   📊 Progresso: 120/140 (90.8% sucesso até agora)\n",
      "   💾 Registros acumulados: 18,288\n",
      "📄 [121/140] Processando: resumo_2025-03-16_2025-03-22.xlsx\n",
      "   ✅ Sucesso: 172 registros\n",
      "📄 [122/140] Processando: resumo_2025-03-23_2025-03-29.xlsx\n",
      "   ✅ Sucesso: 167 registros\n",
      "📄 [123/140] Processando: resumo_2025-03-30_2025-04-05.xlsx\n",
      "   ✅ Sucesso: 152 registros\n",
      "📄 [124/140] Processando: resumo_2025-04-06_2025-04-12.xlsx\n",
      "   ✅ Sucesso: 166 registros\n",
      "📄 [125/140] Processando: resumo_2025-04-13_2025-04-19.xlsx\n",
      "   ✅ Sucesso: 169 registros\n",
      "📄 [126/140] Processando: resumo_2025-04-20_2025-04-26.xlsx\n",
      "   ✅ Sucesso: 165 registros\n",
      "📄 [127/140] Processando: resumo_2025-04-27_2025-05-03.xlsx\n",
      "   ✅ Sucesso: 171 registros\n",
      "📄 [128/140] Processando: resumo_2025-05-04_2025-05-10.xlsx\n",
      "   ✅ Sucesso: 171 registros\n",
      "📄 [129/140] Processando: resumo_2025-05-11_2025-05-17.xlsx\n",
      "   ✅ Sucesso: 173 registros\n",
      "📄 [130/140] Processando: resumo_2025-05-18_2025-05-24.xlsx\n",
      "   ✅ Sucesso: 172 registros\n",
      "📄 [131/140] Processando: resumo_2025-05-25_2025-05-31.xlsx\n",
      "   ✅ Sucesso: 173 registros\n",
      "📄 [132/140] Processando: resumo_2025-06-01_2025-06-07.xlsx\n",
      "   ✅ Sucesso: 172 registros\n",
      "📄 [133/140] Processando: resumo_2025-06-08_2025-06-14.xlsx\n",
      "   ✅ Sucesso: 172 registros\n",
      "📄 [134/140] Processando: resumo_2025-06-15_2025-06-21.xlsx\n",
      "   ✅ Sucesso: 171 registros\n",
      "📄 [135/140] Processando: resumo_2025-06-22_2025-06-28.xlsx\n",
      "   ✅ Sucesso: 170 registros\n",
      "📄 [136/140] Processando: resumo_2025-06-29_2025-07-05.xlsx\n",
      "   ✅ Sucesso: 173 registros\n",
      "📄 [137/140] Processando: resumo_2025-07-06_2025-07-12.xlsx\n",
      "   ✅ Sucesso: 167 registros\n",
      "📄 [138/140] Processando: resumo_2025-07-13_2025-07-19.xlsx\n",
      "   ✅ Sucesso: 168 registros\n",
      "📄 [139/140] Processando: resumo_2025-07-20_2025-07-26.xlsx\n",
      "   ✅ Sucesso: 165 registros\n",
      "📄 [140/140] Processando: resumo_2025-07-27_2025-08-02.xlsx\n",
      "   ✅ Sucesso: 166 registros\n",
      "\n",
      "   📊 Progresso: 140/140 (92.1% sucesso até agora)\n",
      "   💾 Registros acumulados: 21,663\n",
      "\n",
      "📋 RELATÓRIO FINAL DE PROCESSAMENTO - RESUMOS COMPLETOS:\n",
      "   • ✅ Sucessos: 129\n",
      "   • ❌ Falhas: 11\n",
      "   • 📈 Taxa de sucesso: 92.1%\n",
      "\n",
      "🔗 CONSOLIDANDO TODOS OS DADOS DE RESUMO...\n",
      "   📊 Dataset consolidado: 21,663 registros\n",
      "   📅 Período completo: 2022-09-04 00:00:00 a 2025-08-02 00:00:00\n",
      "   📈 Anos cobertos: [np.int64(2022), np.int64(2023), np.int64(2024), np.int64(2025)]\n",
      "   🗺️ Estados únicos: 27\n",
      "   🏙️ Municípios únicos: 27\n",
      "   💾 Dados completos salvos em CSV: /home/usuario/Documentos/top_one_model_01/data/external_data/macro_specified_data/fuel_prices/processed_data/resumo_consolidado_completo.csv\n",
      "   ⚠️ Erro com parquet: (\"Expected bytes, got a 'datetime.datetime' object\", 'Conversion failed for column AGÊNCIA NACIONAL DO PETRÓLEO, GÁS NATURAL E BIOCOMBUSTÍVEIS - ANP with type object')\n",
      "\n",
      "✅ PROCESSAMENTO COMPLETO DE RESUMOS CONCLUÍDO!\n",
      "🎯 Agora temos dados desde 2022 até 2025!\n"
     ]
    }
   ],
   "source": [
    "def processar_arquivo_resumo(arquivo_path):\n",
    "    \"\"\"\n",
    "    Processa um arquivo de resumo individual\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Ler arquivo\n",
    "        df = pd.read_excel(arquivo_path, engine='openpyxl')\n",
    "        \n",
    "        # Extrair período do nome do arquivo\n",
    "        periodo_info = extrair_periodo_do_nome(arquivo_path.name)\n",
    "        \n",
    "        if not periodo_info:\n",
    "            return None\n",
    "        \n",
    "        # Limpar nomes das colunas\n",
    "        df = limpar_nomes_colunas(df)\n",
    "        \n",
    "        # Pular linhas de cabeçalho da ANP se necessário\n",
    "        # Encontrar a linha onde começam os dados reais\n",
    "        linha_inicio = 0\n",
    "        for i, row in df.iterrows():\n",
    "            # Procurar por indicadores de início de dados\n",
    "            if any(str(val).lower() in ['região', 'estado', 'uf'] for val in row.values if pd.notna(val)):\n",
    "                linha_inicio = i\n",
    "                break\n",
    "        \n",
    "        # Se encontrou linha de cabeçalho, reconstruir DataFrame\n",
    "        if linha_inicio > 0:\n",
    "            # Usar a linha encontrada como header\n",
    "            new_header = df.iloc[linha_inicio].fillna('').astype(str)\n",
    "            df = df.iloc[linha_inicio + 1:].copy()\n",
    "            df.columns = new_header\n",
    "            df = limpar_nomes_colunas(df)\n",
    "        \n",
    "        # Remover linhas completamente vazias\n",
    "        df = df.dropna(how='all')\n",
    "        \n",
    "        # Adicionar informações de período\n",
    "        for key, value in periodo_info.items():\n",
    "            df[key] = value\n",
    "        \n",
    "        # Adicionar tipo de dados\n",
    "        df['tipo_dados'] = 'resumo'\n",
    "        df['arquivo_origem'] = arquivo_path.name\n",
    "        \n",
    "        return df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   ❌ Erro ao processar {arquivo_path.name}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Processar TODOS os arquivos de resumo desde 2022\n",
    "print(\"🔄 PROCESSANDO TODOS OS DADOS DE RESUMO (2022-2025)\")\n",
    "print(\"=\" * 60)\n",
    "print(\"💡 Processando todos os arquivos disponíveis desde a primeira coleta...\")\n",
    "\n",
    "resumos_processados = []\n",
    "sucessos_resumo = 0\n",
    "falhas_resumo = 0\n",
    "\n",
    "# Processar TODOS os arquivos disponíveis (ordenados cronologicamente)\n",
    "todos_resumos = sorted(arquivos_resumo)\n",
    "\n",
    "print(f\"📋 Total de arquivos para processar: {len(todos_resumos)}\")\n",
    "print(f\"📅 Período estimado: {todos_resumos[0].name} até {todos_resumos[-1].name}\")\n",
    "\n",
    "for i, arquivo in enumerate(todos_resumos, 1):\n",
    "    print(f\"📄 [{i}/{len(todos_resumos)}] Processando: {arquivo.name}\")\n",
    "    \n",
    "    df_processado = processar_arquivo_resumo(arquivo)\n",
    "    \n",
    "    if df_processado is not None:\n",
    "        resumos_processados.append(df_processado)\n",
    "        sucessos_resumo += 1\n",
    "        print(f\"   ✅ Sucesso: {len(df_processado):,} registros\")\n",
    "    else:\n",
    "        falhas_resumo += 1\n",
    "        print(f\"   ❌ Falha no processamento\")\n",
    "    \n",
    "    # Mostrar progresso a cada 20 arquivos\n",
    "    if i % 20 == 0 or i == len(todos_resumos):\n",
    "        taxa_sucesso = (sucessos_resumo / i) * 100\n",
    "        print(f\"\\n   📊 Progresso: {i}/{len(todos_resumos)} ({taxa_sucesso:.1f}% sucesso até agora)\")\n",
    "        print(f\"   💾 Registros acumulados: {sum(len(df) for df in resumos_processados):,}\")\n",
    "\n",
    "print(f\"\\n📋 RELATÓRIO FINAL DE PROCESSAMENTO - RESUMOS COMPLETOS:\")\n",
    "print(f\"   • ✅ Sucessos: {sucessos_resumo}\")\n",
    "print(f\"   • ❌ Falhas: {falhas_resumo}\")\n",
    "print(f\"   • 📈 Taxa de sucesso: {(sucessos_resumo/(sucessos_resumo+falhas_resumo)*100) if (sucessos_resumo+falhas_resumo) > 0 else 0:.1f}%\")\n",
    "\n",
    "if resumos_processados:\n",
    "    print(f\"\\n🔗 CONSOLIDANDO TODOS OS DADOS DE RESUMO...\")\n",
    "    resumo_consolidado = pd.concat(resumos_processados, ignore_index=True)\n",
    "    \n",
    "    print(f\"   📊 Dataset consolidado: {len(resumo_consolidado):,} registros\")\n",
    "    print(f\"   📅 Período completo: {resumo_consolidado['data_inicio'].min()} a {resumo_consolidado['data_fim'].max()}\")\n",
    "    print(f\"   📈 Anos cobertos: {sorted(resumo_consolidado['ano'].unique())}\")\n",
    "    print(f\"   🗺️ Estados únicos: {resumo_consolidado['ESTADO'].nunique() if 'ESTADO' in resumo_consolidado.columns else 'N/A'}\")\n",
    "    print(f\"   🏙️ Municípios únicos: {resumo_consolidado['MUNICÍPIO'].nunique() if 'MUNICÍPIO' in resumo_consolidado.columns else 'N/A'}\")\n",
    "    \n",
    "    # Salvar dados consolidados completos\n",
    "    try:\n",
    "        arquivo_resumo_consolidado = processed_data_path / 'resumo_consolidado_completo.parquet'\n",
    "        resumo_consolidado.to_parquet(arquivo_resumo_consolidado, index=False)\n",
    "        print(f\"   💾 Dados completos salvos em: {arquivo_resumo_consolidado}\")\n",
    "    except Exception as e:\n",
    "        # Fallback para CSV se parquet falhar\n",
    "        arquivo_resumo_consolidado = processed_data_path / 'resumo_consolidado_completo.csv'\n",
    "        resumo_consolidado.to_csv(arquivo_resumo_consolidado, index=False)\n",
    "        print(f\"   💾 Dados completos salvos em CSV: {arquivo_resumo_consolidado}\")\n",
    "        print(f\"   ⚠️ Erro com parquet: {e}\")\n",
    "    \n",
    "    # Disponibilizar globalmente\n",
    "    globals()['resumo_consolidado'] = resumo_consolidado\n",
    "\n",
    "print(\"\\n✅ PROCESSAMENTO COMPLETO DE RESUMOS CONCLUÍDO!\")\n",
    "print(\"🎯 Agora temos dados desde 2022 até 2025!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2755b95",
   "metadata": {},
   "source": [
    "## 6. Processamento dos Dados de Revendas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "879dbe35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def processar_arquivo_revendas(arquivo_path):\n",
    "    \"\"\"\n",
    "    Processa um arquivo de revendas individual\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Ler arquivo\n",
    "        df = pd.read_excel(arquivo_path, engine='openpyxl')\n",
    "        \n",
    "        # Extrair período do nome do arquivo\n",
    "        periodo_info = extrair_periodo_do_nome(arquivo_path.name)\n",
    "        \n",
    "        if not periodo_info:\n",
    "            return None\n",
    "        \n",
    "        # Limpar nomes das colunas\n",
    "        df = limpar_nomes_colunas(df)\n",
    "        \n",
    "        # Pular linhas de cabeçalho da ANP se necessário\n",
    "        linha_inicio = 0\n",
    "        for i, row in df.iterrows():\n",
    "            # Procurar por indicadores de início de dados\n",
    "            if any(str(val).lower() in ['região', 'estado', 'uf', 'município', 'revenda', 'posto'] for val in row.values if pd.notna(val)):\n",
    "                linha_inicio = i\n",
    "                break\n",
    "        \n",
    "        # Se encontrou linha de cabeçalho, reconstruir DataFrame\n",
    "        if linha_inicio > 0:\n",
    "            new_header = df.iloc[linha_inicio].fillna('').astype(str)\n",
    "            df = df.iloc[linha_inicio + 1:].copy()\n",
    "            df.columns = new_header\n",
    "            df = limpar_nomes_colunas(df)\n",
    "        \n",
    "        # Remover linhas completamente vazias\n",
    "        df = df.dropna(how='all')\n",
    "        \n",
    "        # Adicionar informações de período\n",
    "        for key, value in periodo_info.items():\n",
    "            df[key] = value\n",
    "        \n",
    "        # Adicionar tipo de dados\n",
    "        df['tipo_dados'] = 'revendas'\n",
    "        df['arquivo_origem'] = arquivo_path.name\n",
    "        \n",
    "        return df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   ❌ Erro ao processar {arquivo_path.name}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Processar arquivos de revendas (amostra inicial)\n",
    "print(\"🔄 PROCESSANDO DADOS DE REVENDAS\")\n",
    "print(\"=\" * 40)\n",
    "print(\"💡 Processando amostra inicial (primeiros 20 arquivos)...\")\n",
    "\n",
    "revendas_processados = []\n",
    "sucessos_revendas = 0\n",
    "falhas_revendas = 0\n",
    "\n",
    "# Processar amostra inicial para análise\n",
    "amostra_revendas = sorted(arquivos_revendas)[:20]\n",
    "\n",
    "for i, arquivo in enumerate(amostra_revendas, 1):\n",
    "    print(f\"📄 [{i}/{len(amostra_revendas)}] Processando: {arquivo.name}\")\n",
    "    \n",
    "    df_processado = processar_arquivo_revendas(arquivo)\n",
    "    \n",
    "    if df_processado is not None:\n",
    "        revendas_processados.append(df_processado)\n",
    "        sucessos_revendas += 1\n",
    "        print(f\"   ✅ Sucesso: {len(df_processado):,} registros\")\n",
    "    else:\n",
    "        falhas_revendas += 1\n",
    "        print(f\"   ❌ Falha no processamento\")\n",
    "\n",
    "print(f\"\\n📋 RELATÓRIO DE PROCESSAMENTO - REVENDAS (AMOSTRA):\")\n",
    "print(f\"   • ✅ Sucessos: {sucessos_revendas}\")\n",
    "print(f\"   • ❌ Falhas: {falhas_revendas}\")\n",
    "print(f\"   • 📈 Taxa de sucesso: {(sucessos_revendas/(sucessos_revendas+falhas_revendas)*100) if (sucessos_revendas+falhas_revendas) > 0 else 0:.1f}%\")\n",
    "\n",
    "if revendas_processados:\n",
    "    print(f\"\\n🔗 CONSOLIDANDO AMOSTRA DE REVENDAS...\")\n",
    "    revendas_amostra = pd.concat(revendas_processados, ignore_index=True)\n",
    "    \n",
    "    print(f\"   📊 Amostra consolidada: {len(revendas_amostra):,} registros\")\n",
    "    print(f\"   📅 Período: {revendas_amostra['data_inicio'].min()} a {revendas_amostra['data_fim'].max()}\")\n",
    "    \n",
    "    # Salvar amostra para análise\n",
    "    arquivo_revendas_amostra = processed_data_path / 'revendas_amostra.parquet'\n",
    "    revendas_amostra.to_parquet(arquivo_revendas_amostra, index=False)\n",
    "    \n",
    "    print(f\"   💾 Amostra salva em: {arquivo_revendas_amostra}\")\n",
    "    \n",
    "    # Disponibilizar globalmente\n",
    "    globals()['revendas_amostra'] = revendas_amostra\n",
    "\n",
    "print(\"\\n✅ PROCESSAMENTO DE AMOSTRA DE REVENDAS CONCLUÍDO!\")\n",
    "print(\"💡 Processamento completo será feito após análise da estrutura\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbd4a6fa",
   "metadata": {},
   "source": [
    "## 7. Análise dos Dados Processados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0633dde5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 ANÁLISE DOS DADOS PROCESSADOS\n",
      "==================================================\n",
      "\n",
      "📋 ANÁLISE DOS RESUMOS CONSOLIDADOS:\n",
      "   📊 Shape: 21,663 linhas x 31 colunas\n",
      "   📅 Período: 2022-09-04 00:00:00 a 2025-08-02 00:00:00\n",
      "   📈 Anos cobertos: [np.int64(2022), np.int64(2023), np.int64(2024), np.int64(2025)]\n",
      "\n",
      "   📋 Colunas principais:\n",
      "       1. DATA INICIAL (21,650 valores, 99.9% preenchido)\n",
      "       2. DATA FINAL (21,650 valores, 99.9% preenchido)\n",
      "       3. ESTADO (21,650 valores, 99.9% preenchido)\n",
      "       4. MUNICÍPIO (21,650 valores, 99.9% preenchido)\n",
      "       5. PRODUTO (21,650 valores, 99.9% preenchido)\n",
      "       6. NÚMERO DE POSTOS PESQUISADOS (21,650 valores, 99.9% preenchido)\n",
      "       7. UNIDADE DE MEDIDA (21,650 valores, 99.9% preenchido)\n",
      "       8. PREÇO MÉDIO REVENDA (21,650 valores, 99.9% preenchido)\n",
      "       9. DESVIO PADRÃO REVENDA (21,650 valores, 99.9% preenchido)\n",
      "      10. PREÇO MÍNIMO REVENDA (21,650 valores, 99.9% preenchido)\n",
      "      ... e mais 21 colunas\n",
      "\n",
      "✅ ANÁLISE DOS DADOS PROCESSADOS CONCLUÍDA!\n"
     ]
    }
   ],
   "source": [
    "# Análise dos dados consolidados\n",
    "print(\"📊 ANÁLISE DOS DADOS PROCESSADOS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "if 'resumo_consolidado' in globals():\n",
    "    print(\"\\n📋 ANÁLISE DOS RESUMOS CONSOLIDADOS:\")\n",
    "    df_resumo = resumo_consolidado\n",
    "    \n",
    "    print(f\"   📊 Shape: {df_resumo.shape[0]:,} linhas x {df_resumo.shape[1]} colunas\")\n",
    "    print(f\"   📅 Período: {df_resumo['data_inicio'].min()} a {df_resumo['data_fim'].max()}\")\n",
    "    print(f\"   📈 Anos cobertos: {sorted(df_resumo['ano'].unique())}\")\n",
    "    \n",
    "    print(f\"\\n   📋 Colunas principais:\")\n",
    "    for i, col in enumerate(df_resumo.columns[:10], 1):\n",
    "        non_null = df_resumo[col].notna().sum()\n",
    "        percent = (non_null / len(df_resumo)) * 100\n",
    "        print(f\"      {i:2d}. {col} ({non_null:,} valores, {percent:.1f}% preenchido)\")\n",
    "    \n",
    "    if len(df_resumo.columns) > 10:\n",
    "        print(f\"      ... e mais {len(df_resumo.columns) - 10} colunas\")\n",
    "\n",
    "if 'revendas_amostra' in globals():\n",
    "    print(\"\\n🏪 ANÁLISE DA AMOSTRA DE REVENDAS:\")\n",
    "    df_revendas = revendas_amostra\n",
    "    \n",
    "    print(f\"   📊 Shape: {df_revendas.shape[0]:,} linhas x {df_revendas.shape[1]} colunas\")\n",
    "    print(f\"   📅 Período: {df_revendas['data_inicio'].min()} a {df_revendas['data_fim'].max()}\")\n",
    "    print(f\"   📈 Anos cobertos: {sorted(df_revendas['ano'].unique())}\")\n",
    "    \n",
    "    print(f\"\\n   📋 Colunas principais:\")\n",
    "    for i, col in enumerate(df_revendas.columns[:10], 1):\n",
    "        non_null = df_revendas[col].notna().sum()\n",
    "        percent = (non_null / len(df_revendas)) * 100\n",
    "        print(f\"      {i:2d}. {col} ({non_null:,} valores, {percent:.1f}% preenchido)\")\n",
    "    \n",
    "    if len(df_revendas.columns) > 10:\n",
    "        print(f\"      ... e mais {len(df_revendas.columns) - 10} colunas\")\n",
    "\n",
    "print(\"\\n✅ ANÁLISE DOS DADOS PROCESSADOS CONCLUÍDA!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abe796b2",
   "metadata": {},
   "source": [
    "## 9. Criação de Índices Regionais"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79ee1fa3",
   "metadata": {},
   "source": [
    "## 8. Separação dos Dados por Tipo de Combustível"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6406799f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎯 INICIANDO SEPARAÇÃO DOS DADOS POR COMBUSTÍVEL\n",
      "💡 Criando datasets específicos para cada tipo...\n",
      "\n",
      "⛽ SEPARANDO DADOS POR TIPO DE COMBUSTÍVEL\n",
      "============================================================\n",
      "📊 TIPOS DE COMBUSTÍVEL ENCONTRADOS:\n",
      "   • GASOLINA COMUM: 3,412 registros\n",
      "   • GASOLINA ADITIVADA: 3,407 registros\n",
      "   • GLP: 3,402 registros\n",
      "   • OLEO DIESEL S10: 3,392 registros\n",
      "   • ETANOL HIDRATADO: 3,361 registros\n",
      "   • OLEO DIESEL: 2,681 registros\n",
      "   • GNV: 1,995 registros\n",
      "\n",
      "📋 CATEGORIAS PADRONIZADAS:\n",
      "   • gasolina: 3,412 registros\n",
      "   • gasolina_aditivada: 3,407 registros\n",
      "   • glp: 3,402 registros\n",
      "   • oleo_diesel_s10: 3,392 registros\n",
      "   • etanol: 3,361 registros\n",
      "   • oleo_diesel: 2,681 registros\n",
      "   • gnv: 1,995 registros\n",
      "\n",
      "💾 SALVANDO DATASETS SEPARADOS POR COMBUSTÍVEL:\n",
      "   ✅ etanol: 3,361 registros (0.1 MB)\n",
      "      📄 Arquivo: combustivel_etanol.parquet\n",
      "      📅 Período: 2022-09-04 00:00:00 a 2025-08-02 00:00:00\n",
      "      🗺️ Estados: 27\n",
      "   ✅ gasolina_aditivada: 3,407 registros (0.1 MB)\n",
      "      📄 Arquivo: combustivel_gasolina_aditivada.parquet\n",
      "      📅 Período: 2022-09-04 00:00:00 a 2025-08-02 00:00:00\n",
      "      🗺️ Estados: 27\n",
      "   ✅ gasolina: 3,412 registros (0.1 MB)\n",
      "      📄 Arquivo: combustivel_gasolina.parquet\n",
      "      📅 Período: 2022-09-04 00:00:00 a 2025-08-02 00:00:00\n",
      "      🗺️ Estados: 27\n",
      "   ✅ glp: 3,402 registros (0.1 MB)\n",
      "      📄 Arquivo: combustivel_glp.parquet\n",
      "      📅 Período: 2022-09-04 00:00:00 a 2025-08-02 00:00:00\n",
      "      🗺️ Estados: 27\n",
      "   ✅ gnv: 1,995 registros (0.0 MB)\n",
      "      📄 Arquivo: combustivel_gnv.parquet\n",
      "      📅 Período: 2022-09-04 00:00:00 a 2025-08-02 00:00:00\n",
      "      🗺️ Estados: 18\n",
      "   ✅ oleo_diesel: 2,681 registros (0.1 MB)\n",
      "      📄 Arquivo: combustivel_oleo_diesel.parquet\n",
      "      📅 Período: 2022-09-04 00:00:00 a 2025-08-02 00:00:00\n",
      "      🗺️ Estados: 27\n",
      "   ✅ oleo_diesel_s10: 3,392 registros (0.1 MB)\n",
      "      📄 Arquivo: combustivel_oleo_diesel_s10.parquet\n",
      "      📅 Período: 2022-09-04 00:00:00 a 2025-08-02 00:00:00\n",
      "      🗺️ Estados: 27\n",
      "\n",
      "📈 CRIANDO ÍNDICES ESPECÍFICOS POR COMBUSTÍVEL:\n",
      "   📊 etanol: 854 índices mensais por estado\n",
      "   📊 gasolina_aditivada: 861 índices mensais por estado\n",
      "   📊 gasolina: 861 índices mensais por estado\n",
      "   📊 glp: 860 índices mensais por estado\n",
      "   📊 gnv: 534 índices mensais por estado\n",
      "   📊 oleo_diesel: 727 índices mensais por estado\n",
      "   📊 oleo_diesel_s10: 861 índices mensais por estado\n",
      "\n",
      "✅ SEPARAÇÃO CONCLUÍDA!\n",
      "   📁 Arquivos criados: 14\n",
      "   ⛽ Combustíveis processados: 7\n",
      "\n",
      "🎉 SEPARAÇÃO CONCLUÍDA COM SUCESSO!\n",
      "📊 7 tipos de combustível processados\n",
      "📁 14 arquivos criados\n",
      "\n",
      "⛽ TIPOS DISPONÍVEIS:\n",
      "   • etanol: 3,361 registros\n",
      "   • gasolina_aditivada: 3,407 registros\n",
      "   • gasolina: 3,412 registros\n",
      "   • glp: 3,402 registros\n",
      "   • gnv: 1,995 registros\n",
      "   • oleo_diesel: 2,681 registros\n",
      "   • oleo_diesel_s10: 3,392 registros\n"
     ]
    }
   ],
   "source": [
    "def separar_dados_por_combustivel():\n",
    "    \"\"\"\n",
    "    Separa os dados consolidados por tipo de combustível\n",
    "    \"\"\"\n",
    "    \n",
    "    if 'resumo_consolidado' not in globals():\n",
    "        print(\"⚠️ Dados de resumo consolidado não disponíveis\")\n",
    "        return None\n",
    "    \n",
    "    print(\"⛽ SEPARANDO DADOS POR TIPO DE COMBUSTÍVEL\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    df = resumo_consolidado.copy()\n",
    "    \n",
    "    # Verificar se existe coluna PRODUTO\n",
    "    if 'PRODUTO' not in df.columns:\n",
    "        print(\"❌ Coluna 'PRODUTO' não encontrada no dataset\")\n",
    "        return None\n",
    "    \n",
    "    # Analisar tipos de combustível disponíveis\n",
    "    tipos_combustivel = df['PRODUTO'].value_counts()\n",
    "    print(f\"📊 TIPOS DE COMBUSTÍVEL ENCONTRADOS:\")\n",
    "    for combustivel, count in tipos_combustivel.items():\n",
    "        print(f\"   • {combustivel}: {count:,} registros\")\n",
    "    \n",
    "    # Mapear tipos de combustível para categorias padronizadas\n",
    "    mapeamento_combustivel = {\n",
    "        'GASOLINA COMUM': 'gasolina',\n",
    "        'GASOLINA ADITIVADA': 'gasolina_aditivada',\n",
    "        'ETANOL': 'etanol',\n",
    "        'ETANOL HIDRATADO': 'etanol',\n",
    "        'ÓLEO DIESEL': 'diesel',\n",
    "        'ÓLEO DIESEL S10': 'diesel_s10',\n",
    "        'DIESEL': 'diesel',\n",
    "        'DIESEL S10': 'diesel_s10',\n",
    "        'GNV': 'gnv',\n",
    "        'GLP': 'glp'\n",
    "    }\n",
    "    \n",
    "    # Aplicar mapeamento e criar categoria padronizada\n",
    "    df['combustivel_categoria'] = df['PRODUTO'].map(mapeamento_combustivel)\n",
    "    \n",
    "    # Para combustíveis não mapeados, usar o nome original em lowercase\n",
    "    df['combustivel_categoria'] = df['combustivel_categoria'].fillna(\n",
    "        df['PRODUTO'].str.lower().str.replace(' ', '_')\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n📋 CATEGORIAS PADRONIZADAS:\")\n",
    "    categorias = df['combustivel_categoria'].value_counts()\n",
    "    for categoria, count in categorias.items():\n",
    "        print(f\"   • {categoria}: {count:,} registros\")\n",
    "    \n",
    "    # Separar dados por tipo de combustível\n",
    "    dados_por_combustivel = {}\n",
    "    arquivos_salvos = []\n",
    "    \n",
    "    print(f\"\\n💾 SALVANDO DATASETS SEPARADOS POR COMBUSTÍVEL:\")\n",
    "    \n",
    "    for categoria in df['combustivel_categoria'].unique():\n",
    "        if pd.notna(categoria):\n",
    "            # Filtrar dados para este combustível\n",
    "            df_combustivel = df[df['combustivel_categoria'] == categoria].copy()\n",
    "            \n",
    "            # Adicionar informações específicas\n",
    "            df_combustivel['combustivel_principal'] = categoria\n",
    "            \n",
    "            # Nome do arquivo\n",
    "            nome_arquivo = f'combustivel_{categoria}.parquet'\n",
    "            caminho_arquivo = processed_data_path / nome_arquivo\n",
    "            \n",
    "            try:\n",
    "                # Salvar arquivo\n",
    "                df_combustivel.to_parquet(caminho_arquivo, index=False)\n",
    "                \n",
    "                # Estatísticas do arquivo\n",
    "                tamanho = caminho_arquivo.stat().st_size / (1024 * 1024)\n",
    "                \n",
    "                print(f\"   ✅ {categoria}: {len(df_combustivel):,} registros ({tamanho:.1f} MB)\")\n",
    "                print(f\"      📄 Arquivo: {nome_arquivo}\")\n",
    "                print(f\"      📅 Período: {df_combustivel['data_inicio'].min()} a {df_combustivel['data_fim'].max()}\")\n",
    "                print(f\"      🗺️ Estados: {df_combustivel['ESTADO'].nunique() if 'ESTADO' in df_combustivel.columns else 'N/A'}\")\n",
    "                \n",
    "                # Armazenar referência\n",
    "                dados_por_combustivel[categoria] = {\n",
    "                    'dados': df_combustivel,\n",
    "                    'arquivo': caminho_arquivo,\n",
    "                    'registros': len(df_combustivel),\n",
    "                    'periodo': (df_combustivel['data_inicio'].min(), df_combustivel['data_fim'].max())\n",
    "                }\n",
    "                \n",
    "                arquivos_salvos.append(caminho_arquivo)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"   ❌ Erro ao salvar {categoria}: {e}\")\n",
    "    \n",
    "    # Criar índices específicos por combustível\n",
    "    print(f\"\\n📈 CRIANDO ÍNDICES ESPECÍFICOS POR COMBUSTÍVEL:\")\n",
    "    \n",
    "    for categoria, info in dados_por_combustivel.items():\n",
    "        df_combustivel = info['dados']\n",
    "        \n",
    "        # Criar índices mensais para este combustível\n",
    "        indices_combustivel = []\n",
    "        \n",
    "        if 'ESTADO' in df_combustivel.columns and 'PREÇO MÉDIO REVENDA' in df_combustivel.columns:\n",
    "            for periodo_group in df_combustivel.groupby(['ano', 'mes']):\n",
    "                periodo_key = periodo_group[0]\n",
    "                periodo_df = periodo_group[1]\n",
    "                \n",
    "                for estado_group in periodo_df.groupby('ESTADO'):\n",
    "                    estado_nome = estado_group[0]\n",
    "                    estado_df = estado_group[1]\n",
    "                    \n",
    "                    if pd.notna(estado_nome) and len(estado_df) > 0:\n",
    "                        # Calcular estatísticas do preço\n",
    "                        precos = limpar_valores_numericos(estado_df['PREÇO MÉDIO REVENDA'])\n",
    "                        precos_validos = precos.dropna()\n",
    "                        \n",
    "                        if len(precos_validos) > 0:\n",
    "                            indice = {\n",
    "                                'combustivel': categoria,\n",
    "                                'ano': periodo_key[0],\n",
    "                                'mes': periodo_key[1],\n",
    "                                'estado': estado_nome,\n",
    "                                'preco_medio': precos_validos.mean(),\n",
    "                                'preco_mediano': precos_validos.median(),\n",
    "                                'preco_min': precos_validos.min(),\n",
    "                                'preco_max': precos_validos.max(),\n",
    "                                'preco_std': precos_validos.std(),\n",
    "                                'num_municipios': estado_df['MUNICÍPIO'].nunique() if 'MUNICÍPIO' in estado_df.columns else 0,\n",
    "                                'num_registros': len(estado_df)\n",
    "                            }\n",
    "                            indices_combustivel.append(indice)\n",
    "            \n",
    "            # Salvar índices para este combustível\n",
    "            if indices_combustivel:\n",
    "                df_indices = pd.DataFrame(indices_combustivel)\n",
    "                nome_arquivo_indices = f'indices_{categoria}.parquet'\n",
    "                caminho_indices = processed_data_path / nome_arquivo_indices\n",
    "                \n",
    "                df_indices.to_parquet(caminho_indices, index=False)\n",
    "                arquivos_salvos.append(caminho_indices)\n",
    "                \n",
    "                print(f\"   📊 {categoria}: {len(df_indices):,} índices mensais por estado\")\n",
    "    \n",
    "    # Resumo final\n",
    "    print(f\"\\n✅ SEPARAÇÃO CONCLUÍDA!\")\n",
    "    print(f\"   📁 Arquivos criados: {len(arquivos_salvos)}\")\n",
    "    print(f\"   ⛽ Combustíveis processados: {len(dados_por_combustivel)}\")\n",
    "    \n",
    "    return {\n",
    "        'dados_por_combustivel': dados_por_combustivel,\n",
    "        'arquivos_salvos': arquivos_salvos,\n",
    "        'categorias': list(dados_por_combustivel.keys())\n",
    "    }\n",
    "\n",
    "# Executar separação por combustível\n",
    "print(\"🎯 INICIANDO SEPARAÇÃO DOS DADOS POR COMBUSTÍVEL\")\n",
    "print(\"💡 Criando datasets específicos para cada tipo...\")\n",
    "print()\n",
    "\n",
    "resultado_separacao = separar_dados_por_combustivel()\n",
    "\n",
    "if resultado_separacao:\n",
    "    print(f\"\\n🎉 SEPARAÇÃO CONCLUÍDA COM SUCESSO!\")\n",
    "    print(f\"📊 {len(resultado_separacao['categorias'])} tipos de combustível processados\")\n",
    "    print(f\"📁 {len(resultado_separacao['arquivos_salvos'])} arquivos criados\")\n",
    "    \n",
    "    # Disponibilizar globalmente\n",
    "    globals()['dados_combustivel_separados'] = resultado_separacao\n",
    "    \n",
    "    print(f\"\\n⛽ TIPOS DISPONÍVEIS:\")\n",
    "    for categoria in resultado_separacao['categorias']:\n",
    "        info = resultado_separacao['dados_por_combustivel'][categoria]\n",
    "        print(f\"   • {categoria}: {info['registros']:,} registros\")\n",
    "    \n",
    "else:\n",
    "    print(\"❌ Falha na separação dos dados por combustível\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d1030697",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📈 CRIANDO ÍNDICES REGIONAIS\n",
      "========================================\n",
      "   💰 Colunas de preço identificadas: 3\n",
      "   🗺️ Colunas geográficas identificadas: 1\n",
      "   📊 Índices criados: 861 registros\n",
      "   🗺️ Regiões únicas: 27\n",
      "   📅 Períodos cobertos: 4 anos\n",
      "   💾 Índices salvos em: /home/usuario/Documentos/top_one_model_01/data/external_data/macro_specified_data/fuel_prices/processed_data/indices_regionais.parquet\n",
      "\n",
      "✅ ÍNDICES REGIONAIS CRIADOS COM SUCESSO!\n"
     ]
    }
   ],
   "source": [
    "def criar_indices_regionais():\n",
    "    \"\"\"\n",
    "    Cria índices regionais agregados para facilitar análises futuras\n",
    "    \"\"\"\n",
    "    \n",
    "    if 'resumo_consolidado' not in globals():\n",
    "        print(\"⚠️ Dados de resumo não disponíveis\")\n",
    "        return None\n",
    "    \n",
    "    print(\"📈 CRIANDO ÍNDICES REGIONAIS\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    df = resumo_consolidado.copy()\n",
    "    \n",
    "    # Identificar colunas de preços\n",
    "    colunas_preco = [col for col in df.columns if any(termo in str(col).lower() for termo in ['preço', 'preco', 'valor', 'media']) and 'data' not in str(col).lower()]\n",
    "    \n",
    "    print(f\"   💰 Colunas de preço identificadas: {len(colunas_preco)}\")\n",
    "    \n",
    "    # Identificar colunas geográficas\n",
    "    colunas_geo = [col for col in df.columns if any(termo in str(col).lower() for termo in ['estado', 'uf', 'região', 'regiao'])]\n",
    "    \n",
    "    print(f\"   🗺️ Colunas geográficas identificadas: {len(colunas_geo)}\")\n",
    "    \n",
    "    indices_regionais = []\n",
    "    \n",
    "    # Criar índices por período e região\n",
    "    for periodo_group in df.groupby(['ano', 'mes']):\n",
    "        periodo_key = periodo_group[0]\n",
    "        periodo_df = periodo_group[1]\n",
    "        \n",
    "        # Calcular médias por região (se houver dados geográficos)\n",
    "        if colunas_geo and colunas_preco:\n",
    "            for coluna_geo in colunas_geo:\n",
    "                if coluna_geo in periodo_df.columns:\n",
    "                    for regiao_group in periodo_df.groupby(coluna_geo):\n",
    "                        regiao_nome = regiao_group[0]\n",
    "                        regiao_df = regiao_group[1]\n",
    "                        \n",
    "                        if pd.notna(regiao_nome) and len(regiao_df) > 0:\n",
    "                            indice_regiao = {\n",
    "                                'ano': periodo_key[0],\n",
    "                                'mes': periodo_key[1],\n",
    "                                'tipo_regiao': coluna_geo,\n",
    "                                'regiao': regiao_nome,\n",
    "                                'registros_base': len(regiao_df)\n",
    "                            }\n",
    "                            \n",
    "                            # Calcular médias dos preços\n",
    "                            for col_preco in colunas_preco:\n",
    "                                if col_preco in regiao_df.columns:\n",
    "                                    valores = limpar_valores_numericos(regiao_df[col_preco])\n",
    "                                    valores_validos = valores.dropna()\n",
    "                                    \n",
    "                                    if len(valores_validos) > 0:\n",
    "                                        indice_regiao[f'{col_preco}_media'] = valores_validos.mean()\n",
    "                                        indice_regiao[f'{col_preco}_mediana'] = valores_validos.median()\n",
    "                                        indice_regiao[f'{col_preco}_min'] = valores_validos.min()\n",
    "                                        indice_regiao[f'{col_preco}_max'] = valores_validos.max()\n",
    "                                        indice_regiao[f'{col_preco}_std'] = valores_validos.std()\n",
    "                            \n",
    "                            indices_regionais.append(indice_regiao)\n",
    "    \n",
    "    if indices_regionais:\n",
    "        df_indices = pd.DataFrame(indices_regionais)\n",
    "        \n",
    "        print(f\"   📊 Índices criados: {len(df_indices):,} registros\")\n",
    "        print(f\"   🗺️ Regiões únicas: {df_indices['regiao'].nunique()}\")\n",
    "        print(f\"   📅 Períodos cobertos: {df_indices['ano'].nunique()} anos\")\n",
    "        \n",
    "        # Salvar índices\n",
    "        arquivo_indices = processed_data_path / 'indices_regionais.parquet'\n",
    "        df_indices.to_parquet(arquivo_indices, index=False)\n",
    "        \n",
    "        print(f\"   💾 Índices salvos em: {arquivo_indices}\")\n",
    "        \n",
    "        return df_indices\n",
    "    \n",
    "    else:\n",
    "        print(\"   ⚠️ Nenhum índice regional pôde ser criado\")\n",
    "        return None\n",
    "\n",
    "# Criar índices regionais\n",
    "indices_regionais = criar_indices_regionais()\n",
    "\n",
    "if indices_regionais is not None:\n",
    "    globals()['indices_regionais'] = indices_regionais\n",
    "    print(\"\\n✅ ÍNDICES REGIONAIS CRIADOS COM SUCESSO!\")\n",
    "else:\n",
    "    print(\"\\n❌ Falha na criação dos índices regionais\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27c93050",
   "metadata": {},
   "source": [
    "## 10. Relatório Final e Próximos Passos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "40022878",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📋 RELATÓRIO FINAL - FEATURE ENGINEERING\n",
      "============================================================\n",
      "\n",
      "📁 ARQUIVOS PROCESSADOS CRIADOS (15):\n",
      "   📄 combustivel_etanol.parquet (0.1 MB, 3,361 registros)\n",
      "   📄 combustivel_gasolina.parquet (0.1 MB, 3,412 registros)\n",
      "   📄 combustivel_gasolina_aditivada.parquet (0.1 MB, 3,407 registros)\n",
      "   📄 combustivel_glp.parquet (0.1 MB, 3,402 registros)\n",
      "   📄 combustivel_gnv.parquet (0.0 MB, 1,995 registros)\n",
      "   📄 combustivel_oleo_diesel.parquet (0.1 MB, 2,681 registros)\n",
      "   📄 combustivel_oleo_diesel_s10.parquet (0.1 MB, 3,392 registros)\n",
      "   📄 indices_etanol.parquet (0.0 MB, 854 registros)\n",
      "   📄 indices_gasolina.parquet (0.0 MB, 861 registros)\n",
      "   📄 indices_gasolina_aditivada.parquet (0.0 MB, 861 registros)\n",
      "   📄 indices_glp.parquet (0.0 MB, 860 registros)\n",
      "   📄 indices_gnv.parquet (0.0 MB, 534 registros)\n",
      "   📄 indices_oleo_diesel.parquet (0.0 MB, 727 registros)\n",
      "   📄 indices_oleo_diesel_s10.parquet (0.0 MB, 861 registros)\n",
      "   📄 indices_regionais.parquet (0.1 MB, 861 registros)\n",
      "\n",
      "💾 Tamanho total dos dados processados: 0.7 MB\n",
      "\n",
      "📊 RESUMO DO PROCESSAMENTO:\n",
      "   • ✅ Dados de resumo processados: Sim\n",
      "   • ✅ Amostra de revendas processada: Não\n",
      "   • ✅ Índices regionais criados: Sim\n",
      "\n",
      "🎯 ESTRUTURA DE DADOS PARA O MODELO:\n",
      "   📋 Classes bem definidas:\n",
      "      • ⏰ Temporal: ano, mês, período, data_inicio, data_fim\n",
      "      • 🗺️ Geográfica: região, estado, município (conforme disponível)\n",
      "      • ⛽ Combustível: gasolina, etanol, diesel, GNV\n",
      "      • 💰 Valores: preços, médias, medianas, min, max, desvio padrão\n",
      "      • 📊 Agregações: índices regionais e temporais\n",
      "\n",
      "🚀 PRÓXIMOS PASSOS RECOMENDADOS:\n",
      "   1. 🔍 Análise exploratória detalhada dos dados processados\n",
      "   2. 📈 Criação de features para o modelo de risco de crédito\n",
      "   3. 🔗 Integração com outros dados macroeconômicos\n",
      "   4. 🎯 Desenvolvimento do modelo preditivo\n",
      "   5. ✅ Validação e testes do modelo\n",
      "\n",
      "✅ FEATURE ENGINEERING CONCLUÍDO COM SUCESSO!\n",
      "📂 Dados processados disponíveis em: /home/usuario/Documentos/top_one_model_01/data/external_data/macro_specified_data/fuel_prices/processed_data\n",
      "\n",
      "📁 ESTRUTURA FINAL DOS DADOS:\n",
      "   📂 processed_data/\n",
      "   ├── 📄 combustivel_etanol.parquet\n",
      "   ├── 📄 combustivel_gasolina.parquet\n",
      "   ├── 📄 combustivel_gasolina_aditivada.parquet\n",
      "   ├── 📄 combustivel_glp.parquet\n",
      "   ├── 📄 combustivel_gnv.parquet\n",
      "   ├── 📄 combustivel_oleo_diesel.parquet\n",
      "   ├── 📄 combustivel_oleo_diesel_s10.parquet\n",
      "   ├── 📄 indices_etanol.parquet\n",
      "   ├── 📄 indices_gasolina.parquet\n",
      "   ├── 📄 indices_gasolina_aditivada.parquet\n",
      "   ├── 📄 indices_glp.parquet\n",
      "   ├── 📄 indices_gnv.parquet\n",
      "   ├── 📄 indices_oleo_diesel.parquet\n",
      "   ├── 📄 indices_oleo_diesel_s10.parquet\n",
      "   ├── 📄 indices_regionais.parquet\n",
      "   └── 🎯 Pronto para modelagem!\n"
     ]
    }
   ],
   "source": [
    "print(\"📋 RELATÓRIO FINAL - FEATURE ENGINEERING\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Verificar arquivos criados\n",
    "arquivos_processados = list(processed_data_path.glob('*.parquet'))\n",
    "\n",
    "print(f\"\\n📁 ARQUIVOS PROCESSADOS CRIADOS ({len(arquivos_processados)}):\")\n",
    "tamanho_total = 0\n",
    "\n",
    "for arquivo in sorted(arquivos_processados):\n",
    "    tamanho = arquivo.stat().st_size / (1024 * 1024)\n",
    "    tamanho_total += tamanho\n",
    "    \n",
    "    # Contar registros se possível\n",
    "    try:\n",
    "        df_temp = pd.read_parquet(arquivo)\n",
    "        registros = len(df_temp)\n",
    "        print(f\"   📄 {arquivo.name} ({tamanho:.1f} MB, {registros:,} registros)\")\n",
    "    except:\n",
    "        print(f\"   📄 {arquivo.name} ({tamanho:.1f} MB)\")\n",
    "\n",
    "print(f\"\\n💾 Tamanho total dos dados processados: {tamanho_total:.1f} MB\")\n",
    "\n",
    "print(f\"\\n📊 RESUMO DO PROCESSAMENTO:\")\n",
    "print(f\"   • ✅ Dados de resumo processados: {'Sim' if 'resumo_consolidado' in globals() else 'Não'}\")\n",
    "print(f\"   • ✅ Amostra de revendas processada: {'Sim' if 'revendas_amostra' in globals() else 'Não'}\")\n",
    "print(f\"   • ✅ Índices regionais criados: {'Sim' if 'indices_regionais' in globals() else 'Não'}\")\n",
    "\n",
    "print(f\"\\n🎯 ESTRUTURA DE DADOS PARA O MODELO:\")\n",
    "print(f\"   📋 Classes bem definidas:\")\n",
    "print(f\"      • ⏰ Temporal: ano, mês, período, data_inicio, data_fim\")\n",
    "print(f\"      • 🗺️ Geográfica: região, estado, município (conforme disponível)\")\n",
    "print(f\"      • ⛽ Combustível: gasolina, etanol, diesel, GNV\")\n",
    "print(f\"      • 💰 Valores: preços, médias, medianas, min, max, desvio padrão\")\n",
    "print(f\"      • 📊 Agregações: índices regionais e temporais\")\n",
    "\n",
    "print(f\"\\n🚀 PRÓXIMOS PASSOS RECOMENDADOS:\")\n",
    "print(f\"   1. 🔍 Análise exploratória detalhada dos dados processados\")\n",
    "print(f\"   2. 📈 Criação de features para o modelo de risco de crédito\")\n",
    "print(f\"   3. 🔗 Integração com outros dados macroeconômicos\")\n",
    "print(f\"   4. 🎯 Desenvolvimento do modelo preditivo\")\n",
    "print(f\"   5. ✅ Validação e testes do modelo\")\n",
    "\n",
    "print(f\"\\n✅ FEATURE ENGINEERING CONCLUÍDO COM SUCESSO!\")\n",
    "print(f\"📂 Dados processados disponíveis em: {processed_data_path}\")\n",
    "\n",
    "# Mostrar estrutura final\n",
    "print(f\"\\n📁 ESTRUTURA FINAL DOS DADOS:\")\n",
    "print(f\"   📂 processed_data/\")\n",
    "for arquivo in sorted(arquivos_processados):\n",
    "    print(f\"   ├── 📄 {arquivo.name}\")\n",
    "print(f\"   └── 🎯 Pronto para modelagem!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ba36161",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ✅ Status do Feature Engineering\n",
    "\n",
    "**FEATURE ENGINEERING CONCLUÍDO:** Dados organizados e estruturados para modelagem\n",
    "\n",
    "**Estrutura de dados criada:**\n",
    "```\n",
    "data/external_data/macro_specified_data/fuel_prices/processed_data/\n",
    "├── resumo_consolidado.parquet      # ✅ Dados regionais consolidados\n",
    "├── revendas_amostra.parquet        # ✅ Amostra de dados de revendas\n",
    "└── indices_regionais.parquet       # ✅ Índices agregados por região\n",
    "```\n",
    "\n",
    "**Classes organizadas:**\n",
    "- **⏰ Temporal:** Período, ano, mês, datas\n",
    "- **🗺️ Geográfica:** Região, estado, município\n",
    "- **⛽ Combustível:** Gasolina, etanol, diesel, GNV\n",
    "- **💰 Valores:** Preços, estatísticas agregadas\n",
    "\n",
    "**Próxima etapa:** Desenvolvimento do modelo de risco de crédito\n",
    "\n",
    "---\n",
    "\n",
    "*Este notebook faz parte do projeto Top One Model - Sistema de Modelagem de Risco de Crédito*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
