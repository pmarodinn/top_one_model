{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4d025324",
   "metadata": {},
   "source": [
    "# üìä Top One Model - An√°lise do Dataset Interno\n",
    "\n",
    "## Objetivo\n",
    "Este notebook realiza a an√°lise explorat√≥ria inicial do dataset interno de cr√©dito com 700.000+ registros para entender:\n",
    "- Estrutura e qualidade dos dados dispon√≠veis\n",
    "- Features internas existentes \n",
    "- Distribui√ß√µes e padr√µes nos dados\n",
    "- Identifica√ß√£o de vari√°veis geogr√°ficas para associa√ß√£o com dados macro regionais\n",
    "\n",
    "## Estrutura do Projeto\n",
    "```\n",
    "‚îú‚îÄ‚îÄ data/\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ internal_data/          # Dataset interno (.xlsx)\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ external/              # Dados de web scraping\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ processed/             # Dados processados\n",
    "‚îú‚îÄ‚îÄ src/\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ data_collection/       # M√≥dulos de web scraping\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ data_engineering/      # Engenharia de features\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ modeling/              # Modelos de ML\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ validation/            # Valida√ß√£o e testes\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ prediction/            # Sistema de predi√ß√£o\n",
    "‚îî‚îÄ‚îÄ notebooks/                 # An√°lises explorat√≥rias\n",
    "```\n",
    "\n",
    "**Autores:** Pedro Schuves Marodin, Enzo Holtzmann Gaio  \n",
    "**Data:** Agosto 2025"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfe9491d",
   "metadata": {},
   "source": [
    "## 1. Environment Setup and Library Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbeff7fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment Setup\n",
    "import sys\n",
    "import os\n",
    "sys.path.append('../src')\n",
    "\n",
    "# Data Manipulation and Analysis\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "# Visualization Libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Web Scraping Libraries (for later use)\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "\n",
    "# Machine Learning Libraries \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "\n",
    "# Configuration and Utilities\n",
    "import yaml\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Display Settings\n",
    "pd.set_option('display.max_columns', 100)\n",
    "pd.set_option('display.max_rows', 20)\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"‚úÖ Todas as bibliotecas importadas com sucesso!\")\n",
    "print(f\"üìç Diret√≥rio atual: {os.getcwd()}\")\n",
    "print(f\"üêç Vers√£o Python: {sys.version}\")\n",
    "print(f\"üêº Vers√£o Pandas: {pd.__version__}\")\n",
    "print(f\"üìä Vers√£o NumPy: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08de44fa",
   "metadata": {},
   "source": [
    "## 2. Data Loading and Initial Exploration\n",
    "\n",
    "Vamos carregar o dataset interno e fazer uma primeira an√°lise da estrutura dos dados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43b1f6ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir caminho para o dataset interno\n",
    "DATASET_PATH = \"../data/internal_data/dataset_interno_top_one.xlsx\"\n",
    "\n",
    "try:\n",
    "    # Carregar o dataset\n",
    "    print(\"üì• Carregando dataset interno...\")\n",
    "    df_internal = pd.read_excel(DATASET_PATH)\n",
    "    \n",
    "    print(f\"‚úÖ Dataset carregado com sucesso!\")\n",
    "    print(f\"üìä Dimens√µes: {df_internal.shape[0]:,} linhas x {df_internal.shape[1]} colunas\")\n",
    "    print(f\"üíæ Tamanho em mem√≥ria: {df_internal.memory_usage(deep=True).sum() / (1024**2):.2f} MB\")\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(f\"‚ùå Arquivo n√£o encontrado em: {DATASET_PATH}\")\n",
    "    print(\"üìù Por favor, certifique-se de que o arquivo est√° no local correto!\")\n",
    "    df_internal = None\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Erro ao carregar o dataset: {str(e)}\")\n",
    "    df_internal = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ab308da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# An√°lise inicial do dataset se carregado com sucesso\n",
    "if df_internal is not None:\n",
    "    print(\"=\" * 60)\n",
    "    print(\"üìä AN√ÅLISE INICIAL DO DATASET\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Informa√ß√µes b√°sicas\n",
    "    print(f\"üìã Colunas dispon√≠veis ({len(df_internal.columns)}):\")\n",
    "    for i, col in enumerate(df_internal.columns, 1):\n",
    "        print(f\"  {i:2d}. {col}\")\n",
    "    \n",
    "    print(f\"\\nüìà Primeiras 5 linhas do dataset:\")\n",
    "    display(df_internal.head())\n",
    "    \n",
    "    print(f\"\\nüìä Informa√ß√µes sobre tipos de dados:\")\n",
    "    display(df_internal.info())\n",
    "    \n",
    "    print(f\"\\nüîç Estat√≠sticas descritivas:\")\n",
    "    display(df_internal.describe())\n",
    "else:\n",
    "    print(\"‚è≥ Aguardando carregamento do dataset...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e2e53e4",
   "metadata": {},
   "source": [
    "## 3. An√°lise Geogr√°fica e Identifica√ß√£o de Cidades\n",
    "\n",
    "An√°lise espec√≠fica para identificar colunas geogr√°ficas que ser√£o usadas para associa√ß√£o com dados regionais coletados via web scraping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5057ec64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# An√°lise geogr√°fica - identificar colunas de cidade/munic√≠pio\n",
    "if df_internal is not None:\n",
    "    print(\"üåç AN√ÅLISE GEOGR√ÅFICA\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Buscar colunas que podem conter informa√ß√µes geogr√°ficas\n",
    "    geo_keywords = ['cidade', 'municipio', 'city', 'municipality', 'local', 'endereco', 'cep']\n",
    "    possible_geo_cols = []\n",
    "    \n",
    "    for col in df_internal.columns:\n",
    "        if any(keyword in col.lower() for keyword in geo_keywords):\n",
    "            possible_geo_cols.append(col)\n",
    "    \n",
    "    print(f\"üìç Colunas geogr√°ficas identificadas: {possible_geo_cols}\")\n",
    "    \n",
    "    # Analisar cada coluna geogr√°fica\n",
    "    for col in possible_geo_cols:\n",
    "        print(f\"\\nüîç An√°lise da coluna: {col}\")\n",
    "        print(f\"   - Tipo: {df_internal[col].dtype}\")\n",
    "        print(f\"   - Valores √∫nicos: {df_internal[col].nunique()}\")\n",
    "        print(f\"   - Valores faltantes: {df_internal[col].isnull().sum()} ({df_internal[col].isnull().mean()*100:.1f}%)\")\n",
    "        \n",
    "        if df_internal[col].dtype == 'object':\n",
    "            # Mostrar algumas cidades mais frequentes\n",
    "            top_cities = df_internal[col].value_counts().head(10)\n",
    "            print(f\"   - Top 10 cidades:\")\n",
    "            for city, count in top_cities.items():\n",
    "                print(f\"     ‚Ä¢ {city}: {count:,} registros\")\n",
    "    \n",
    "    # Se encontrou colunas geogr√°ficas, criar visualiza√ß√£o\n",
    "    if possible_geo_cols:\n",
    "        city_col = possible_geo_cols[0]  # Usar primeira coluna encontrada\n",
    "        \n",
    "        # Distribui√ß√£o geogr√°fica\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        \n",
    "        plt.subplot(1, 2, 1)\n",
    "        top_20_cities = df_internal[city_col].value_counts().head(20)\n",
    "        top_20_cities.plot(kind='bar')\n",
    "        plt.title(f'Top 20 Cidades - {city_col}')\n",
    "        plt.xticks(rotation=45, ha='right')\n",
    "        plt.ylabel('N√∫mero de Registros')\n",
    "        \n",
    "        plt.subplot(1, 2, 2)\n",
    "        # Distribui√ß√£o percentual\n",
    "        city_distribution = df_internal[city_col].value_counts()\n",
    "        others_count = city_distribution[20:].sum() if len(city_distribution) > 20 else 0\n",
    "        \n",
    "        plot_data = city_distribution.head(10).copy()\n",
    "        if others_count > 0:\n",
    "            plot_data['Outras'] = others_count\n",
    "        \n",
    "        plt.pie(plot_data.values, labels=plot_data.index, autopct='%1.1f%%')\n",
    "        plt.title('Distribui√ß√£o Geogr√°fica (Top 10 + Outras)')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        print(f\"\\nüìä Resumo geogr√°fico:\")\n",
    "        print(f\"   - Total de cidades √∫nicas: {df_internal[city_col].nunique()}\")\n",
    "        print(f\"   - Cidade mais frequente: {df_internal[city_col].mode().iloc[0]} ({df_internal[city_col].value_counts().iloc[0]:,} registros)\")\n",
    "        print(f\"   - Cobertura geogr√°fica: {(1-df_internal[city_col].isnull().mean())*100:.1f}%\")\n",
    "    \n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è Nenhuma coluna geogr√°fica clara identificada.\")\n",
    "        print(\"üí° Sugest√£o: Verifique se existe coluna com informa√ß√µes de localiza√ß√£o.\")\n",
    "else:\n",
    "    print(\"‚è≥ Dataset n√£o carregado ainda.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
